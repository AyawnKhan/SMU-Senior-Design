{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# FinQA — Hallucination Detection via Uncertainty Quantification\n",
    "**Part A | Owners: Ayan Khan & Anum Khan**\n",
    "\n",
    "This notebook walks through three UQ methods to detect hallucinations in financial reasoning:\n",
    "1. **Self-Consistency** — sample N answers, measure agreement\n",
    "2. **LogProb Entropy** — measure token-level uncertainty via logprobs\n",
    "3. **Verbalized Confidence** — ask the model to self-report confidence\n",
    "\n",
    "Then we check: *does high confidence actually correlate with correctness?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once to install dependencies\n",
    "# %pip install openai datasets pandas matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, json, time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from LLM_utils import (\n",
    "    LLMConfig, LLMRequest, SimpleDiskCache, \n",
    "    generate, self_consistency_samples, simple_majority_vote,\n",
    "    build_finqa_prompt, parse_json_answer, append_jsonl, DEFAULT_SYSTEM,\n",
    "    make_delimited_prompt, with_retries\n",
    ")\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # loads .env into os.environ automatically\n",
    "\n",
    "HF_API_KEY = os.getenv('HF_API_KEY')  \n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "MODELS: Dict[str, LLMConfig] = {\n",
    "    # Free HuggingFace Inference API models\n",
    "    'mistral-7b':  LLMConfig(provider='huggingface', model='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.2, max_tokens=512),\n",
    "    'llama-3':     LLMConfig(provider='huggingface', model='meta-llama/Meta-Llama-3-8B-Instruct', temperature=0.2, max_tokens=512),\n",
    "    'mock':        LLMConfig(provider='mock',        model='mock', temperature=0.2, max_tokens=512),\n",
    "}\n",
    "\n",
    "SELECTED_MODELS = ['mistral-7b', 'mock']  # ← swap / add models here\n",
    "MAX_ROWS        = 10    # number of FinQA examples to evaluate\n",
    "N_SC_SAMPLES    = 5     # self-consistency samples per example\n",
    "RESULTS_PATH    = 'runs/exploration_results.jsonl'\n",
    "CACHE_PATH      = 'runs/llm_cache.jsonl'\n",
    "\n",
    "cache = SimpleDiskCache(CACHE_PATH)\n",
    "print('Config ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-data-header",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 rows from data/finQA/test.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ETR/2016/page_23.pdf-2</td>\n",
       "      <td>what is the net change in net revenue during 2...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTC/2015/page_41.pdf-4</td>\n",
       "      <td>what percentage of total facilities as measure...</td>\n",
       "      <td>14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADI/2011/page_61.pdf-2</td>\n",
       "      <td>what is the percentage change in cash flow hed...</td>\n",
       "      <td>9.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIS/2010/page_70.pdf-2</td>\n",
       "      <td>what portion of total purchase price is relate...</td>\n",
       "      <td>2.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAS/2017/page_27.pdf-2</td>\n",
       "      <td>what was the difference in percentage cumulati...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                           question  \\\n",
       "0   ETR/2016/page_23.pdf-2  what is the net change in net revenue during 2...   \n",
       "1  INTC/2015/page_41.pdf-4  what percentage of total facilities as measure...   \n",
       "2   ADI/2011/page_61.pdf-2  what is the percentage change in cash flow hed...   \n",
       "3   FIS/2010/page_70.pdf-2  what portion of total purchase price is relate...   \n",
       "4   MAS/2017/page_27.pdf-2  what was the difference in percentage cumulati...   \n",
       "\n",
       "  gold_answer  \n",
       "0          94  \n",
       "1         14%  \n",
       "2        9.9%  \n",
       "3        2.9%  \n",
       "4              "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(split: str = 'test', max_rows: int = 10) -> List[Dict]:\n",
    "    path = f'data/finQA/{split}.json' # Load data from local finQA dataset\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for i, ex in enumerate(raw[:max_rows]):\n",
    "        pre   = ' '.join(ex.get('pre_text',  []) or [])\n",
    "        post  = ' '.join(ex.get('post_text', []) or [])\n",
    "        table = '\\n'.join(' | '.join(str(c) for c in r) for r in (ex.get('table') or []))\n",
    "        rows.append({\n",
    "            'id':          ex.get('id', f'{split}_{i}'),\n",
    "            'split':       split,\n",
    "            'question':    ex['qa']['question'],\n",
    "            'context':     f'{pre}\\n\\nTable:\\n{table}\\n\\n{post}'.strip(),\n",
    "            'gold_answer': str(ex['qa']['answer']),\n",
    "        })\n",
    "\n",
    "    print(f'Loaded {len(rows)} rows from {path}')\n",
    "    return rows\n",
    "\n",
    "rows = load_data(split='test', max_rows=MAX_ROWS)\n",
    "pd.DataFrame(rows)[['id', 'question', 'gold_answer']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-uq-header",
   "metadata": {},
   "source": [
    "## 2. UQ Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-uq-sc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Consistency defined\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Self-Consistency\n",
    "\"\"\"\n",
    "Core idea: ask the model the same question N times and see if it keeps giving the same answer. \n",
    "High agreement = high confidence. \n",
    "Disagreement = likely hallucinating.\n",
    "\"\"\"\n",
    "\n",
    "def uq_self_consistency(system, user, cfg, n=5, cache=None):\n",
    "    # return agreement ratio as confidence\n",
    "    samples = self_consistency_samples(system, user, cfg, n=n, cache=cache)\n",
    "    answers = [str(parse_json_answer(s.text).get('final_answer', '')).strip() for s in samples]\n",
    "    majority, counts = simple_majority_vote(answers)\n",
    "    return {\n",
    "        'method':          'self_consistency',\n",
    "        'confidence':      round(counts.get(majority, 0) / max(len(answers), 1), 4),\n",
    "        'majority_answer': majority,\n",
    "        'vote_counts':     counts,\n",
    "    }\n",
    "\n",
    "print('Self-Consistency defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdfce2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:     self_consistency\n",
      "Confidence: 1.0\n",
      "Answer:     42\n",
      "Votes:      {'42': 3}\n"
     ]
    }
   ],
   "source": [
    "# Quick test — uses mock provider so no API calls\n",
    "test_cfg    = MODELS['mock']\n",
    "test_row    = rows[0]\n",
    "test_user   = build_finqa_prompt(test_row['context'], test_row['question'])\n",
    "test_result = uq_self_consistency(DEFAULT_SYSTEM, test_user, test_cfg, n=3, cache=None)\n",
    "\n",
    "print(f\"Method:     {test_result['method']}\")\n",
    "print(f\"Confidence: {test_result['confidence']}\")\n",
    "print(f\"Answer:     {test_result['majority_answer']}\")\n",
    "print(f\"Votes:      {test_result['vote_counts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-uq-lpe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogProb Entropy defined\n"
     ]
    }
   ],
   "source": [
    "def uq_logprob_entropy(system, user, cfg, cache=None):\n",
    "    \"\"\"\n",
    "    Core idea: Instead of asking the model multiple times, we look inside a single response at \n",
    "    how certain the model was about each word it chose. \n",
    "    High certainty per word = high confidence.\n",
    "    \"\"\"\n",
    "    if cfg.provider != 'openai':\n",
    "        return {'method': 'logprob_entropy', 'confidence': None, 'mean_entropy': None}\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "\n",
    "        def _call():\n",
    "            return client.chat.completions.create(\n",
    "                model=cfg.model,\n",
    "                messages=[{'role': 'system', 'content': system}, {'role': 'user', 'content': user}],\n",
    "                temperature=0.0, max_tokens=cfg.max_tokens, logprobs=True, top_logprobs=5,\n",
    "            )\n",
    "\n",
    "        resp    = with_retries(_call, max_retries=cfg.max_retries)  # from LLM_utils\n",
    "        lp_data = resp.choices[0].logprobs.content or []\n",
    "        entropies = []\n",
    "        for tok in lp_data:\n",
    "            probs = [math.exp(t.logprob) for t in (tok.top_logprobs or [])]\n",
    "            if probs:\n",
    "                total = sum(probs)\n",
    "                probs = [p / total for p in probs]\n",
    "                entropies.append(-sum(p * math.log(p + 1e-12) for p in probs))\n",
    "        mean_h = sum(entropies) / len(entropies) if entropies else None\n",
    "        return {\n",
    "            'method':       'logprob_entropy',\n",
    "            'mean_entropy': round(mean_h, 6) if mean_h else None,\n",
    "            'confidence':   round(1.0 - mean_h / math.log(5), 4) if mean_h else None,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'method': 'logprob_entropy', 'confidence': None, 'mean_entropy': None, 'error': str(e)}\n",
    "\n",
    "print('LogProb Entropy defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc770620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:       logprob_entropy\n",
      "Confidence:   None\n",
      "Mean Entropy: None\n",
      "Note: returns None for non-OpenAI providers\n"
     ]
    }
   ],
   "source": [
    "# Test — logprob entropy returns None for non-openai providers (expected)\n",
    "test_cfg = MODELS['mock']\n",
    "test_row = rows[0]\n",
    "test_user = build_finqa_prompt(test_row['context'], test_row['question'])\n",
    "\n",
    "test_result = uq_logprob_entropy(DEFAULT_SYSTEM, test_user, test_cfg)\n",
    "\n",
    "print(f\"Method:       {test_result['method']}\")\n",
    "print(f\"Confidence:   {test_result['confidence']}\")   # expect None — mock isn't openai\n",
    "print(f\"Mean Entropy: {test_result['mean_entropy']}\") # expect None\n",
    "print(f\"Note: returns None for non-OpenAI providers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-uq-vc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbalized Confidence defined\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Verbalized Confidence\n",
    "def uq_verbalized_confidence(context, question, cfg, cache=None):\n",
    "    \"\"\"Core idea: just ask the model how confident it is. \"\"\"\n",
    "    task = (\n",
    "        'Solve the financial question step-by-step.\\n'\n",
    "        'Output ONLY this JSON on the last line (no extra keys):\\n'\n",
    "        '{\"final_answer\": \"...\", \"confidence_pct\": <0-100>, \"reasoning\": \"...\"}'\n",
    "    )\n",
    "    user_prompt = make_delimited_prompt(task, f'CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}')\n",
    "    req  = LLMRequest(system='You are a careful financial reasoning assistant.',\n",
    "                      user=user_prompt, meta={'uq': 'verbalized'}, config=cfg)\n",
    "    resp = generate(req, cache=cache)\n",
    "    parsed   = parse_json_answer(resp.text)\n",
    "    conf_raw = parsed.get('confidence_pct')\n",
    "    return {\n",
    "        'method':       'verbalized_confidence',\n",
    "        'confidence':   round(float(conf_raw) / 100.0, 4) if conf_raw is not None else None,\n",
    "        'final_answer': str(parsed.get('final_answer', '')).strip(),\n",
    "    }\n",
    "\n",
    "print('Verbalized Confidence defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1987ddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:       verbalized_confidence\n",
      "Confidence:   None\n",
      "Final Answer: 42\n",
      "Note: mock doesn't return confidence_pct field\n"
     ]
    }
   ],
   "source": [
    "# Test — verbalized confidence\n",
    "test_cfg  = MODELS['mock']\n",
    "test_row  = rows[0]\n",
    "\n",
    "test_result = uq_verbalized_confidence(test_row['context'], test_row['question'], test_cfg, cache=None)\n",
    "\n",
    "print(f\"Method:       {test_result['method']}\")\n",
    "print(f\"Confidence:   {test_result['confidence']}\")    # None — mock returns plain string, not valid JSON with confidence_pct\n",
    "print(f\"Final Answer: {test_result['final_answer']}\")  # None — same reason\n",
    "print(f\"Note: mock doesn't return confidence_pct field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pipeline-header",
   "metadata": {},
   "source": [
    "## 3. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-correctness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline helpers defined\n"
     ]
    }
   ],
   "source": [
    "def is_correct(predicted: str, gold: str) -> bool:\n",
    "    # Fuzzy match: strips white space,symbols,lowercase then compares .\n",
    "    def norm(s):\n",
    "        return re.sub(r'[\\s\\$,%]+', '', s.lower().strip())\n",
    "    p, g = norm(predicted), norm(gold)\n",
    "    return p == g or g in p or p in g\n",
    "\n",
    "def run_example(row, model_name, cfg, n_sc=5, cache=None):\n",
    "    #Run all three UQ methods on one FinQA example.\n",
    "    user_prompt = build_finqa_prompt(row['context'], row['question'])\n",
    "\n",
    "    # Baseline greedy answer\n",
    "    baseline = generate(\n",
    "        LLMRequest(system=DEFAULT_SYSTEM, user=user_prompt,\n",
    "                   meta={'id': row['id'], 'model': model_name}, config=cfg),\n",
    "        cache=cache\n",
    "    )\n",
    "    baseline_answer = str(parse_json_answer(baseline.text).get('final_answer', '')).strip()\n",
    "\n",
    "    sc  = uq_self_consistency(DEFAULT_SYSTEM, user_prompt, cfg, n=n_sc, cache=cache)\n",
    "    lpe = uq_logprob_entropy(DEFAULT_SYSTEM, user_prompt, cfg, cache=cache)\n",
    "    vc  = uq_verbalized_confidence(row['context'], row['question'], cfg, cache=cache)\n",
    "\n",
    "    return {\n",
    "        'id': row['id'], 'model': model_name,\n",
    "        'question': row['question'], 'gold': row['gold_answer'],\n",
    "        'predicted': baseline_answer,\n",
    "        'correct': is_correct(baseline_answer, row['gold_answer']),\n",
    "        'sc_confidence':  sc['confidence'],\n",
    "        'lpe_confidence': lpe.get('confidence'),\n",
    "        'vc_confidence':  vc.get('confidence'),\n",
    "        'vc_answer':      vc.get('final_answer'),\n",
    "    }\n",
    "\n",
    "print('Pipeline helpers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== mistral-7b (10 examples) ===\n",
      "  [ERROR] ETR/2016/page_23.pdf-2: 'InferenceClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/SMU-Senior-Design/LLM_utils.py:122\u001b[0m, in \u001b[0;36mwith_retries\u001b[0;34m(fn, max_retries)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/GitHub/SMU-Senior-Design/LLM_utils.py:226\u001b[0m, in \u001b[0;36mgenerate.<locals>._do\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do\u001b[39m():\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m provider_call(req\u001b[38;5;241m.\u001b[39msystem, req\u001b[38;5;241m.\u001b[39muser, req\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/Documents/GitHub/SMU-Senior-Design/LLM_utils.py:188\u001b[0m, in \u001b[0;36mprovider_call\u001b[0;34m(system, user, cfg)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_huggingface(system, user, cfg)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/SMU-Senior-Design/LLM_utils.py:171\u001b[0m, in \u001b[0;36mcall_huggingface\u001b[0;34m(system, user, cfg)\u001b[0m\n\u001b[1;32m    170\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 171\u001b[0m resp \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    172\u001b[0m     model\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    173\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    174\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system},\n\u001b[1;32m    175\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user},\n\u001b[1;32m    176\u001b[0m     ],\n\u001b[1;32m    177\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m    178\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m    179\u001b[0m )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InferenceClient' object has no attribute 'chat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m         r \u001b[38;5;241m=\u001b[39m run_example(row, model_name, cfg, n_sc\u001b[38;5;241m=\u001b[39mN_SC_SAMPLES, cache\u001b[38;5;241m=\u001b[39mcache)\n\u001b[1;32m      9\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[1;32m     10\u001b[0m         append_jsonl(RESULTS_PATH, r)\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mrun_example\u001b[0;34m(row, model_name, cfg, n_sc, cache)\u001b[0m\n\u001b[1;32m     10\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m build_finqa_prompt(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Baseline greedy answer\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m baseline \u001b[38;5;241m=\u001b[39m generate(\n\u001b[1;32m     14\u001b[0m     LLMRequest(system\u001b[38;5;241m=\u001b[39mDEFAULT_SYSTEM, user\u001b[38;5;241m=\u001b[39muser_prompt,\n\u001b[1;32m     15\u001b[0m                meta\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model_name}, config\u001b[38;5;241m=\u001b[39mcfg),\n\u001b[1;32m     16\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m baseline_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(parse_json_answer(baseline\u001b[38;5;241m.\u001b[39mtext)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_answer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     20\u001b[0m sc  \u001b[38;5;241m=\u001b[39m uq_self_consistency(DEFAULT_SYSTEM, user_prompt, cfg, n\u001b[38;5;241m=\u001b[39mn_sc, cache\u001b[38;5;241m=\u001b[39mcache)\n",
      "File \u001b[0;32m~/Documents/GitHub/SMU-Senior-Design/LLM_utils.py:228\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(req, cache)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do\u001b[39m():\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m provider_call(req\u001b[38;5;241m.\u001b[39msystem, req\u001b[38;5;241m.\u001b[39muser, req\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m--> 228\u001b[0m raw \u001b[38;5;241m=\u001b[39m with_retries(_do, max_retries\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_retries)\n\u001b[1;32m    229\u001b[0m text \u001b[38;5;241m=\u001b[39m extract_text(raw)\n\u001b[1;32m    230\u001b[0m usage \u001b[38;5;241m=\u001b[39m extract_usage(raw)\n",
      "File \u001b[0;32m~/Documents/GitHub/SMU-Senior-Design/LLM_utils.py:128\u001b[0m, in \u001b[0;36mwith_retries\u001b[0;34m(fn, max_retries)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#exponential backoff + jitter\u001b[39;00m\n\u001b[1;32m    127\u001b[0m sleep_s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m attempt) \u001b[38;5;241m+\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom()\n\u001b[0;32m--> 128\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(sleep_s)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name in SELECTED_MODELS:\n",
    "    cfg = MODELS[model_name]\n",
    "    print(f'\\n=== {model_name} ({len(rows)} examples) ===')\n",
    "    for row in rows:\n",
    "        try:\n",
    "            r = run_example(row, model_name, cfg, n_sc=N_SC_SAMPLES, cache=cache)\n",
    "            results.append(r)\n",
    "            append_jsonl(RESULTS_PATH, r)\n",
    "            status = '✓' if r['correct'] else '✗'\n",
    "            print(f\"  {status} [{row['id']}] predicted={r['predicted']!r}  gold={row['gold_answer']!r}  sc={r['sc_confidence']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] {row['id']}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f'\\nDone. {len(df)} results saved to {RESULTS_PATH}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-analysis-header",
   "metadata": {},
   "source": [
    "## 4. Calibration Analysis\n",
    "Key hypothesis: **high confidence → more likely correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-calibration-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.7\n",
    "methods = [('sc_confidence', 'Self-Consistency'), ('lpe_confidence', 'LogProb Entropy'), ('vc_confidence', 'Verbalized Conf')]\n",
    "\n",
    "rows_summary = []\n",
    "for col, label in methods:\n",
    "    sub = df.dropna(subset=[col])\n",
    "    if sub.empty: continue\n",
    "    hi = sub[sub[col] >= THRESHOLD]\n",
    "    lo = sub[sub[col] <  THRESHOLD]\n",
    "    rows_summary.append({\n",
    "        'Method': label,\n",
    "        'High-conf n': len(hi), 'High-conf acc': f\"{hi['correct'].mean():.0%}\" if len(hi) else 'N/A',\n",
    "        'Low-conf n':  len(lo), 'Low-conf acc':  f\"{lo['correct'].mean():.0%}\" if len(lo) else 'N/A',\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows_summary).set_index('Method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-calibration-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "fig.suptitle('Confidence vs Correctness by UQ Method', fontsize=13)\n",
    "\n",
    "for ax, (col, label) in zip(axes, methods):\n",
    "    sub = df.dropna(subset=[col])\n",
    "    if sub.empty:\n",
    "        ax.set_title(f'{label}\\n(no data)')\n",
    "        continue\n",
    "    # Bin into 5 buckets, plot mean accuracy per bucket\n",
    "    sub = sub.copy()\n",
    "    sub['bin'] = pd.cut(sub[col], bins=5)\n",
    "    grouped = sub.groupby('bin', observed=True)['correct'].mean()\n",
    "    grouped.plot(kind='bar', ax=ax, color='steelblue', edgecolor='white')\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel('Confidence bucket')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('runs/calibration_plot.png', dpi=120)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS7324-ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
